{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "In this task you will train an SVM and a neural network, which will be able to predict how much you will like a given song. This you will do by training on two sets of data: one set of songs you like, and one set of songs you dislike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "**Task:** If you have not already done so, head back to dataset_config.py and uncomment 10-15 playlists representing genres you do do not like. Do this is in the list stored in annoying_songs. The greater the dataset, the better, but we do not want to spend too much time setting up the dataset in this workshop. The playlists contain different amount of songs. Try to end up with approximately 1500-2000 songs in the dataset, and a 50/50 distribution with likeable and annoying songs. When you run the data pre-processing cell, the number of songs in each set will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "nbimporter.options['only_defs'] = False\n",
    "import importlib\n",
    "\n",
    "import dataset_config\n",
    "import features_config\n",
    "from helpers_and_data_parsing import DIR_PLAYLIST_FEATURES, filter_features, split_dataset, label_songs, remove_duplicates, remove_training_set_tracks, load_pickle, plot_training_history\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "from sklearn import svm\n",
    "import pickle\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "np.random.seed(6)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DATA PRE-PROCESSING\n",
    "'''\n",
    "def get_pre_processed_data():\n",
    "    importlib.reload(dataset_config)\n",
    "    # Remove duplicates\n",
    "    x_likeable = remove_duplicates(dataset_config.likeable_songs)\n",
    "    x_annoying = remove_duplicates(dataset_config.annoying_songs)\n",
    "\n",
    "    print(\"Number of likeable songs in training dataset %i\" % len(x_likeable))\n",
    "    print(\"Number of annoying songs in training dataset %i\\n\" % len(x_annoying))\n",
    "\n",
    "    # Filter out chosen features and omit metadata such as song name and artist\n",
    "    x_likeable_data = filter_features(x_likeable)\n",
    "    x_annoying_data = filter_features(x_annoying)\n",
    "\n",
    "    # Assign labels\n",
    "    y_likeable = label_songs(x_likeable_data, 1)\n",
    "    y_annoying = label_songs(x_annoying, 0)\n",
    "\n",
    "    # Create list containing entire dataset input\n",
    "    x_data = np.array(x_likeable_data + x_annoying_data)\n",
    "    y_labels = np.array(y_likeable + y_annoying)\n",
    "\n",
    "    # Normalize input training data\n",
    "    x_data = StandardScaler().fit_transform(x_data)\n",
    "\n",
    "    # Randomize data and split into training, validation and test set\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = split_dataset(x_data, y_labels, 0.1, 0.1)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "_ = get_pre_processed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "We are going for an easy start by setting up an out-of-the-box SVM, which is only a few lines of code using the scikit-learn framework. You will get to play around a little more with the neural network.\n",
    "\n",
    "**Tasks:** In the following cells are some todos. You will have to finish the code to run the SVM.\n",
    "1. Instantiate an SVM object with this line: svm.SVC(probability=True), and store in the clf variable.\n",
    "\n",
    "2. Train the SVM by calling .fit() on the SVM object and pass the training set (x_train) and labels (y_train) as arguments.\n",
    "\n",
    "3. Store the prediction results on the validation set in val_score by calling .score() and passing the validation set and labels as arguments. \n",
    "\n",
    "4. Run the cell and you will get prediction accuracies on the training set and validation set. The model is automatically after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train SVM model\n",
    "'''\n",
    "importlib.reload(features_config)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_pre_processed_data()\n",
    "\n",
    "# TODO: Instantiate SVM object using Sklearn framework\n",
    "clf = None\n",
    "\n",
    "# TODO: Train SVM by calling fit() and passing training set + labels ass arguments\n",
    "\n",
    "# TODO: Output accuracy on validation set by calling score() and passing validation set + label as arguments\n",
    "val_score = \"\"\n",
    "\n",
    "# TODO: Output accuracy on validation set by calling score() and passing validation set + label as arguments\n",
    "test_score = \"\"\n",
    "\n",
    "print('Validation accuracy of SVM: ' + str(val_score))\n",
    "print('Test accuracy of SVM: ' + str(test_score))\n",
    "\n",
    "filehandler = open('./svm_model.pkl', 'wb')\n",
    "pickle.dump(clf, filehandler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music recommendation\n",
    "\n",
    "Result-wise, this is where we get to the interesting part. Now we are going to use the model to output recommendations. For this, we have downloaded a large playlist containing around 8000 songs from different genres. In this context, we will use this to represent the “all the songs in the world” (we know it’s not). Unlike Spotify, we do not have simple access to the actual library of all songs in the world. We are going to run this playlist through our trained model. The model will then give us a probability score describing how likely it is that we will like the each song, based on patterns it captures. \n",
    "\n",
    "**Task:** Run the cell. The output will show 30 songs the model thinks you will like. For fun, it also shows you 30 songs you will definitely dislike with passion. Do the results seem to be right based on the songs you recognize (or don’t recognize)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_recommendation_svm():\n",
    "    # Training set\n",
    "    x_likeable = remove_duplicates(dataset_config.likeable_songs)\n",
    "    x_annoying = remove_duplicates(dataset_config.annoying_songs)\n",
    "    \n",
    "    world_songs = load_pickle(DIR_PLAYLIST_FEATURES + '/Mega List/world_songs.pkl')\n",
    "\n",
    "    # Pre-process songs, as was done for training set\n",
    "    world_songs_without_duplicates = remove_duplicates(world_songs)\n",
    "\n",
    "    # Must also remove training set songs from world songs\n",
    "    training_set = x_likeable + x_annoying\n",
    "    world_songs_without_duplicates = remove_training_set_tracks(training_set, world_songs_without_duplicates)\n",
    "\n",
    "    filehandler = open('./svm_model.pkl', 'rb')\n",
    "    clf = pickle.load(filehandler)\n",
    "\n",
    "    all_data = StandardScaler().fit_transform(np.array(filter_features(world_songs_without_duplicates)))\n",
    "    y_pred_probability = clf.predict_proba(all_data)\n",
    "    y_pred = clf.predict(all_data)\n",
    "\n",
    "    # Find 30 songs, which are most likeable\n",
    "    song_pred_mapping = [{\n",
    "        'name': world_songs_without_duplicates[i]['name'],\n",
    "        'artists': world_songs_without_duplicates[i]['artists'],\n",
    "        'likeability_score': y_pred_probability[i][1],\n",
    "    } for i in range(len(y_pred))]\n",
    "\n",
    "    # Sort songs by likeability score\n",
    "    song_pred_mapping_sorted_by_likeability = sorted(song_pred_mapping, key=lambda x: x['likeability_score'])\n",
    "\n",
    "    # Extract 30 most likeable songs and 30 most annoying songs\n",
    "    recommendations = list(reversed(song_pred_mapping_sorted_by_likeability[-30:]))\n",
    "    thirty_most_annoying_songs = song_pred_mapping_sorted_by_likeability[:30]\n",
    "\n",
    "    # Print most likeable and annoying songs\n",
    "    print(\"\\n------Your music recommendations-------\")\n",
    "    print(pd.DataFrame(recommendations)[['artists', 'name', 'likeability_score']])\n",
    "\n",
    "    print(\"\\n-------Songs you will definitely hate-------\")\n",
    "    print(pd.DataFrame(thirty_most_annoying_songs)[['artists', 'name', 'likeability_score']])\n",
    "    print(\"\")\n",
    "music_recommendation_svm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "**Task:** Below is a simple implementation of a neural network. Run the cell to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train neural network model\n",
    "'''\n",
    "importlib.reload(features_config)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_pre_processed_data()\n",
    "### TRAINING ###\n",
    "# Create model\n",
    "\n",
    "# TODO: Here you can modify the architecture of the neural network model and experiment with different parameters\n",
    "save_model=False\n",
    "model = Sequential()\n",
    "model.add(Dense(1,  # TODO: Number of hidden layer neurons\n",
    "                input_dim=len(features_config.get_features()),\n",
    "                activation='relu'))\n",
    "\n",
    "# TODO: Possible to add additional neural network layers here\n",
    "# TODO: Use model.add(Dense(number of hidden layer neurons, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# TODO: Optional; add early stopping as callback\n",
    "history = model.fit(x=x_train,\n",
    "                    y=y_train,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    batch_size=50,  # Number of data samples to run through network before parameter update\n",
    "                    epochs=50,  # TODO: Number of times to run entire training set through network\n",
    "                    shuffle=True,\n",
    "                    callbacks=[]\n",
    "                    ).history\n",
    "\n",
    "score = model.evaluate(x_test, y_test, batch_size=50)  # Evaluate model on test set\n",
    "print('Test loss:%f' % (score[0]))\n",
    "print('Test accuracy:%f' % (score[1]))\n",
    "\n",
    "if save_model:\n",
    "    model.save('./nn_model.h5')\n",
    "    print(\"Model saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**: \n",
    "The following cell will display the training graphs.\n",
    "\n",
    "1. Try out different parameter values and architectures for the neural network. For example, you can make the model more complex by adding hidden layers. This you can do by adding Dense() layers to the model. You can also choose a larger number of neurons in the hidden layers.\n",
    "\n",
    "\n",
    "2. Study the outputted graphs while trying different values. They show the amount of loss (error) and accuracy for the training set and validation set during training. Do you see any changes, for the better or worse, as a result of your decisions?\n",
    "    - The graphs may not say much before you train the models for more than one epoch. Crank it up, friend.\n",
    "    - The number of hidden neurons to choose is not straightforward, but there are some rules of thumb. Usually it should be somewhere between the size of the input layer and the size of the output layer\n",
    "    \n",
    "    The initial setup only has one neuron in a single hidden layer. Try increasing this number. Does your model learn more?\n",
    "    \n",
    "    \n",
    "3. When you are happy with your model, you can save it by changing the argument for the save_model variable from False to True, and re-running the training cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def music_recommendation_nn(model_path='./nn_model.h5'):\n",
    "    importlib.reload(dataset_config)\n",
    "    # Training set\n",
    "    x_likeable = remove_duplicates(dataset_config.likeable_songs)\n",
    "    x_annoying = remove_duplicates(dataset_config.annoying_songs)\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    world_songs = load_pickle(DIR_PLAYLIST_FEATURES + '/Mega List/world_songs.pkl')\n",
    "\n",
    "    # Pre-process songs, as was done for training set\n",
    "    world_songs_without_duplicates = remove_duplicates(world_songs)\n",
    "\n",
    "    # Must also remove training set songs from world songs\n",
    "    training_set = x_likeable + x_annoying\n",
    "    world_songs_without_duplicates = remove_training_set_tracks(training_set, world_songs_without_duplicates)\n",
    "\n",
    "    x_data = StandardScaler().fit_transform(np.array(filter_features(world_songs_without_duplicates)))\n",
    "    y_pred = model.predict(x_data, batch_size=50)\n",
    "\n",
    "    # Find 30 songs, which are most likeable\n",
    "    song_pred_mapping = [{\n",
    "        'name': world_songs_without_duplicates[i]['name'],\n",
    "        'artists': world_songs_without_duplicates[i]['artists'],\n",
    "        'likeability_score': y_pred[i],\n",
    "    } for i in range(len(y_pred))]\n",
    "\n",
    "    # Sort songs by likeability score\n",
    "    song_pred_mapping_sorted_by_likeability = sorted(song_pred_mapping, key=lambda x: x['likeability_score'])\n",
    "\n",
    "    # Extract 30 most likeable songs and 30 most annoying songs\n",
    "    recommendations = song_pred_mapping_sorted_by_likeability[-30:]\n",
    "    thirty_most_annoying_songs = song_pred_mapping_sorted_by_likeability[:30]\n",
    "\n",
    "    # Print most likeable and annoying songs\n",
    "    print(\"\\n------Your music recommendations-------\")\n",
    "    print(pd.DataFrame(recommendations)[['artists', 'name', 'likeability_score']])\n",
    "\n",
    "    print(\"\\n-------Songs you will definitely hate-------\")\n",
    "    print(pd.DataFrame(thirty_most_annoying_songs)[['artists', 'name', 'likeability_score']])\n",
    "    print(\"\")\n",
    "\n",
    "music_recommendation_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional neural network tasks\n",
    "Already done, huh? Try out these neat tricks to train a better model.\n",
    "#### Dropout\n",
    "By adding more layers or neurons, your model expands its capacity of capturing patterns in the training set, but this may lead to overfitting. You can counteract this by adding a Dropout layer after your hidden layer. This will omit a certain percentage of the neurons in your hidden layer in every iteration. Practically speaking, this simulates the use of several neural networks for training, and thus reduces the chance of overfitting. \n",
    "\n",
    "Use dropout by adding an instance of Dropout(rate) to you model, exactly as you have done with Dense layers earlier.\n",
    "\n",
    "\n",
    "#### Early stopping\n",
    "Depending on the configuration and your architecture, you may see from your training graphs that if you train for many epochs the performance on the validation set stops at a certain point. If this is the model architecture you are going for, then it would be a good idea to stop training at the point where performance stops to improve. Do this by using early stopping, which monitors a certain value and stops training when that value stops to improve. \n",
    "\n",
    "Use early stopping by adding an instance of EarlyStopping(monitor=”val_loss”, patience=2) in the list of callbacks for your model. This will monitor the validation loss and stop training if it does not improve over two epochs, thus saving the best possible model following this configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
