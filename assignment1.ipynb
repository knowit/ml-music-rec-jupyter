{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1: Clustering\n",
    "\n",
    "Let us start looking at some real machine learning algorithms. In this task weâ€™re going to cluster the dataset, and see some natural groups formed based on the similarity of songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "nbimporter.options['only_defs'] = False\n",
    "import importlib\n",
    "%matplotlib notebook\n",
    "\n",
    "import dataset_config\n",
    "from helpers_and_data_parsing import pre_process_cluster_data, remove_duplicates, print_kmeans, print_dbscan, print_agglomerative\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "**Task:** The following cells contain the implementation of k-means. Run the cells to load and run the algorithm. Try running the algorithm with different parameters. E.g., see what happens when you change the number of clusters to construct. There is a TODO where you can change the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_kmeans_on_reduced_data(reduced_data):\n",
    "    \"\"\"\n",
    "    Runs the kmeans clustering algorithm on the reduced data.\n",
    "    :param reduced_data: All features gathered from the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .01  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 0.5, reduced_data[:, 0].max() + 0.5\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 0.5, reduced_data[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # TODO: Change parameters for the reduced data kmeans algorithm here\n",
    "    kmeans = KMeans(n_clusters=2, init='k-means++', n_init=10,\n",
    "                    max_iter=300, tol=0.0001, precompute_distances='auto',\n",
    "                    verbose=0, random_state=None, copy_x=True, n_jobs=1,\n",
    "                    algorithm='auto')\n",
    "\n",
    "    kmeans.fit(X=reduced_data, y=None)\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    return Z.reshape(xx.shape), kmeans, x_min, x_max, y_min, y_max, xx, yy, kmeans.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans():\n",
    "    \"\"\"\n",
    "    Runs the kmeans clustering algorithm.\n",
    "    :param X_data: All features gathered from the dataset in form of (n_features, n_songs)\n",
    "    :param reduced_data: The result of the PCA-reduced data on the original data.\n",
    "    :param X_songname: Corresponding songname\n",
    "    :param X_artist: Corresponding artist\n",
    "    \"\"\"\n",
    "    importlib.reload(dataset_config)\n",
    "    \n",
    "    # For printing to console\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "    # Getting all audio features from my likeable songs\n",
    "    audio_features = dataset_config.likeable_songs\n",
    "\n",
    "    # Remove duplicates\n",
    "    audio_features = remove_duplicates(audio_features)\n",
    "\n",
    "    # X_data is the features you have chosen.\n",
    "    X_data, X_songname, X_artist = pre_process_cluster_data(audio_features)\n",
    "\n",
    "    # Simply normalize all data\n",
    "    X_data = StandardScaler().fit_transform(X_data)\n",
    "\n",
    "    # X_data's dimensions are reduced to n_components.\n",
    "    reduced_data = PCA(n_components=2).fit_transform(X_data)\n",
    "\n",
    "    # Clustering based on the original data. Gives more precise results, but can't be visualized\n",
    "    # kmeans_y_labels = k_means.cluster_with_kmeans_on_original_data(X_data)\n",
    "    # print(kmeans_y_labels)\n",
    "\n",
    "    # Clustering on PCA-reduced data. Can be visualized, but gives more unprecise results\n",
    "    Z, kmeans, x_min, x_max, y_min, y_max, xx, yy, X_labels = cluster_with_kmeans_on_reduced_data(reduced_data)\n",
    "    print_kmeans(Z, xx, yy, reduced_data, X_songname, X_artist, X_labels, kmeans, x_min, x_max, y_min, y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks**: \n",
    "1. The following cell will run K-means and display the clusters. Study the graph and see if you see anything interesting. From your perspective, do the clusters make sense? Use the mouse pointer and hover over the data points to display song information. \n",
    "\n",
    "2. If you have not already done so, hand-pick the features you would like to use for clustering songs. Try different selections and see if this affects your clusters in any way. Does any of the features seem to make more impact that the others. Please do compare and discuss with your comrades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run KMeans clustering\n",
    "run_kmeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "**Task**: The following cells contain the implementation for DBSCAN. Run similar experiments for DBSCAN as you did for K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_dbscan(data):\n",
    "    \"\"\"\n",
    "    Runs the dbscan clustering algorithm.\n",
    "    :param data: All features gathered from the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Change parameters from here\n",
    "    db = DBSCAN(eps=0.34, min_samples=15, metric='euclidean',\n",
    "                metric_params=None, algorithm='auto', leaf_size=30,\n",
    "                p=None, n_jobs=1).fit(data)\n",
    "\n",
    "    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "    core_samples_mask[db.core_sample_indices_] = True\n",
    "\n",
    "    labels = db.labels_\n",
    "\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    return n_clusters_, labels, core_samples_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dbscan():\n",
    "    \"\"\"\n",
    "    Runs the dbscan clustering algorithm.\n",
    "    :param X_data: All features gathered from the dataset in form of (n_features, n_songs)\n",
    "    :param reduced_data: The result of the PCA-reduced data on the original data.\n",
    "    :param X_songname: Corresponding songname\n",
    "    :param X_artist: Corresponding artist\n",
    "    \"\"\"\n",
    "    importlib.reload(dataset_config)\n",
    "    \n",
    "    # For printing to console\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "    # Getting all audio features from my likeable songs\n",
    "    audio_features = dataset_config.likeable_songs\n",
    "\n",
    "    # Remove duplicates\n",
    "    audio_features = remove_duplicates(audio_features)\n",
    "\n",
    "    # X_data is the features you have chosen.\n",
    "    X_data, X_songname, X_artist = pre_process_cluster_data(audio_features)\n",
    "\n",
    "    # Simply normalize all data\n",
    "    X_data = StandardScaler().fit_transform(X_data)\n",
    "\n",
    "    # X_data's dimensions are reduced to n_components.\n",
    "    reduced_data = PCA(n_components=2).fit_transform(X_data)\n",
    "\n",
    "    # Clustering based on the original data. Gives more precise results, but can't be visualized\n",
    "    n_clusters_, labels, core_samples_mask = cluster_with_dbscan(X_data)\n",
    "    # print(labels)\n",
    "\n",
    "    # Clustering on PCA-reduced data. Can be visualized, but gives more unprecise results\n",
    "    n_clusters_, labels, core_samples_mask = cluster_with_dbscan(reduced_data)\n",
    "    print_dbscan(labels, reduced_data, n_clusters_, core_samples_mask, X_songname, X_artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DBSCAN clustering\n",
    "run_dbscan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchical Clustering\n",
    "\n",
    "**Task:** The following cells contain the implementation for agglomerative hierarchical clustering. Run similar experiments here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_agglomerative(data):\n",
    "    \"\"\"\n",
    "    Runs the hierarchical agglomerative clustering algorithm.\n",
    "    :param data: All features gathered from the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Change parameters from here\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=1, affinity='euclidean', memory=None,\n",
    "                                            connectivity=None, compute_full_tree='auto',\n",
    "                                            linkage='ward', pooling_func=np.mean)\n",
    "\n",
    "    agglomerative.fit(X=data, y=None)\n",
    "\n",
    "    return agglomerative.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agglomerative():\n",
    "    \"\"\"\n",
    "    Runs the agglomerative clustering algorithm.\n",
    "    :param X_data: All features gathered from the dataset in form of (n_features, n_songs)\n",
    "    :param reduced_data: The result of the PCA-reduced data on the original data.\n",
    "    :param X_songname: Corresponding songname\n",
    "    :param X_artist: Corresponding artist\n",
    "    \"\"\"\n",
    "    importlib.reload(dataset_config)\n",
    "    \n",
    "    # For printing to console\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "    # Getting all audio features from my likeable songs\n",
    "    audio_features = dataset_config.likeable_songs\n",
    "\n",
    "    # Remove duplicates\n",
    "    audio_features = remove_duplicates(audio_features)\n",
    "\n",
    "    # X_data is the features you have chosen.\n",
    "    X_data, X_songname, X_artist = pre_process_cluster_data(audio_features)\n",
    "\n",
    "    # Simply normalize all data\n",
    "    X_data = StandardScaler().fit_transform(X_data)\n",
    "\n",
    "    # X_data's dimensions are reduced to n_components.\n",
    "    reduced_data = PCA(n_components=2).fit_transform(X_data)\n",
    "\n",
    "    # Clustering based on the original data. Gives more precise results, but can't be visualized\n",
    "    # agglomerative_y_labels = agglomerative.cluster_with_agglomerative(X_data)\n",
    "    # print(agglomerative_y_labels)\n",
    "\n",
    "    # Clustering on PCA-reduced data. Can be visualized, but gives more unprecise results\n",
    "    X_labels = cluster_with_agglomerative(reduced_data)\n",
    "    print_agglomerative(reduced_data, X_labels, X_songname, X_artist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Agglomerative clustering\n",
    "run_agglomerative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
